---
layout: page
title: Introduction
subtitle:  Risk of Tageted financial advertising on META
cover-img: /assets/img/path.jpg
thumbnail-img: /assets/img/thumb.png
share-img: /assets/img/path.jpg
---

Advertising platforms such as Meta have become key channels through which users access financial service information, including loans, insurance, and credit scoring. According to Meta’s own data, more than two billion people use Facebook every month, making it a vital platform for businesses to reach potential customers. Meta operates a closed advertising ecosystem that serves only its own platform users, claiming to support advertisers throughout the entire customer lifecycle and thereby increasing marketing efficiency.
 
However, this advertising infrastructure is far from neutral. Meta’s ad system allows advertisers to narrow their target audience by attributes such as interests, gender, and location. Its algorithms prioritize users who are predicted to be more likely to engage with the ad. While this personalization ostensibly improves efficiency, it also has the potential to systematically shape who has access to what kind of information, raising concerns about structural inequality.
 
When audience targeting includes sensitive demographic or behavioral data, the risk of discrimination increases. Studies have shown that even when advertisers specify broad or neutral targeting parameters, the platform’s delivery algorithm can lead to skewed distribution across demographic lines. This is especially problematic in socially sensitive domains such as housing, employment, and financial services, where such biases may violate anti-discrimination laws. For instance, a 2016 ProPublica investigation revealed that Facebook has enabled advertisers to exclude users based on “ethnic affinities” from seeing housing ads. The U.S. Department of Housing and Urban Development later filed a lawsuit against Facebook, accusing the platform of “encouraging, enabling, and causing” discriminatory practices.
 
In the European context, these concerns fall under the regulatory scope of the General Data Protection Regulation (GDPR), which grants users the right to be informed about how their data is used for personalized advertising (Articles 13 and 14). GDPR also mandates that personal data must be processed lawfully, fairly, and transparently (Article 5). If a platform uses opaque optimization mechanisms to influence ad exposure without informing users, particularly in the context of financial products, it may be in breach of these principles.
 
More critically, algorithmic bias in ad delivery not only leads to unequal access to information, but may also exacerbate financial risk. Research shows that products such as payday loans and high-interest credit offers are disproportionately shown to women and lower-income users. Due to disparities in financial literacy, digital access, and representation in training data, women are often more vulnerable to being steered toward riskier financial products. These patterns reflect not only gender bias but also a potential amplification of existing economic precarity through algorithmic decision-making.
 
Against this backdrop, our study focuses on the French context to explore whether Meta’s ad delivery system exhibits structural bias based on gender and age in the distribution of loan-related advertising. Specifically, we ask: Are certain demographic groups unintentionally more frequently exposed to financial product advertisements that carry risk—and is this pattern driven by the platform’s own delivery algorithms?

